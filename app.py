"""
Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù…Ø°ÙƒØ±Ø§Øª Ø§Ù„ØªØ®Ø±Ø¬ - Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±
Ø§Ù„Ø¥ØµØ¯Ø§Ø±: 2.4 - Ù…Ø¹ Ø¯Ø¹Ù… Word Generator
Ø§Ù„ØªØ§Ø±ÙŠØ®: Ø¯ÙŠØ³Ù…Ø¨Ø± 2024

Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:
- Ø¥Ø¶Ø§ÙØ© endpoint Ø¬Ø¯ÙŠØ¯ /analyze_word Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ù„ÙØ§Øª Word
- Ø¯Ø¹Ù… ÙƒØ§Ù…Ù„ Ù„Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ø³Ø¨Ø¹Ø©
- ØªÙ†Ø³ÙŠÙ‚ Ø§Ø­ØªØ±Ø§ÙÙŠ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠØ©
"""

from flask import Flask, request, jsonify, send_file
import pandas as pd
import numpy as np
from scipy import stats
import statsmodels.api as sm
from datetime import datetime
import requests
from io import BytesIO
import re
import traceback
import tempfile
import os

# Import Word Generator
from spss_word_generator import SPSSWordGenerator

app = Flask(__name__)

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ù…Ø§Ù†
app.config['MAX_CONTENT_LENGTH'] = 10 * 1024 * 1024  # 10MB max


class FileHandler:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª - ØªØ­Ù…ÙŠÙ„ Ù…Ù† Google Drive"""
    
    def load_file(self, file_source):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ù…Ù† Google Drive Ø£Ùˆ Ø£ÙŠ Ù…ØµØ¯Ø±"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø±Ø§Ø¨Ø· Google Drive
            if 'drive.google.com' in file_source or 'docs.google.com' in file_source:
                file_source = self._convert_gdrive_url(file_source)
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù
            response = requests.get(file_source, timeout=30)
            response.raise_for_status()
            file_content = BytesIO(response.content)
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
            if '.csv' in file_source.lower() or 'csv' in file_source.lower():
                df = pd.read_csv(file_content, encoding='utf-8-sig')
            else:
                df = pd.read_excel(file_content)
            
            # ØªÙ†Ø¸ÙŠÙ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø¨Ø´ÙƒÙ„ Ø´Ø§Ù…Ù„
            import unicodedata
            clean_cols = []
            for c in df.columns:
                # ØªØ·Ø¨ÙŠØ¹ Unicode
                new = unicodedata.normalize("NFKC", str(c))
                # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ØºÙŠØ± Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ÙƒØ³Ø± ÙˆØ§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®ÙÙŠØ©
                new = new.replace("\u00A0", " ").strip()
                new = new.replace("\u200f", "").replace("\u200e", "").strip()
                # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
                new = " ".join(new.split())
                clean_cols.append(new)
            
            df.columns = clean_cols
            
            return df
            
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {str(e)}")
            return None
    
    def _convert_gdrive_url(self, url):
        """ØªØ­ÙˆÙŠÙ„ Ø±Ø§Ø¨Ø· Google Drive Ù„Ù„ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±"""
        file_id = None
        
        # Ø§Ù„Ù†Ù…Ø· 1: /file/d/FILE_ID/
        match = re.search(r'/file/d/([a-zA-Z0-9_-]+)', url)
        if match:
            file_id = match.group(1)
        
        # Ø§Ù„Ù†Ù…Ø· 2: id=FILE_ID
        if not file_id:
            match = re.search(r'id=([a-zA-Z0-9_-]+)', url)
            if match:
                file_id = match.group(1)
        
        # Ø§Ù„Ù†Ù…Ø· 3: /d/FILE_ID/ (Google Sheets)
        if not file_id:
            match = re.search(r'/d/([a-zA-Z0-9_-]+)', url)
            if match:
                file_id = match.group(1)
        
        if file_id:
            return f"https://drive.google.com/uc?export=download&id={file_id}"
        
        return url


class DescriptiveAnalyzer:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØµÙÙŠ"""
    
    def __init__(self, dataframe):
        self.df = dataframe
    
    def run_analysis(self):
        """ØªØ­Ù„ÙŠÙ„ ÙˆØµÙÙŠ ÙƒØ§Ù…Ù„ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª"""
        results = {
            "Ù…ØªØºÙŠØ±Ø§Øª_Ø±Ù‚Ù…ÙŠØ©": [],
            "Ù…ØªØºÙŠØ±Ø§Øª_ÙØ¦ÙˆÙŠØ©": []
        }
        
        for column in self.df.columns:
            try:
                if pd.api.types.is_numeric_dtype(self.df[column]):
                    # Ù…ØªØºÙŠØ± Ø±Ù‚Ù…ÙŠ
                    data = self.df[column].dropna()
                    if len(data) > 0:
                        results["Ù…ØªØºÙŠØ±Ø§Øª_Ø±Ù‚Ù…ÙŠØ©"].append({
                            "Ø§Ù„Ù…ØªØºÙŠØ±": column,
                            "Ø§Ù„Ø¹Ø¯Ø¯": int(len(data)),
                            "Ø§Ù„Ù…ØªÙˆØ³Ø·": round(float(data.mean()), 2),
                            "Ø§Ù„ÙˆØ³ÙŠØ·": round(float(data.median()), 2),
                            "Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù_Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ": round(float(data.std()), 2),
                            "Ø£ØµØºØ±_Ù‚ÙŠÙ…Ø©": round(float(data.min()), 2),
                            "Ø£ÙƒØ¨Ø±_Ù‚ÙŠÙ…Ø©": round(float(data.max()), 2)
                        })
                else:
                    # Ù…ØªØºÙŠØ± ÙØ¦ÙˆÙŠ
                    data = self.df[column].dropna()
                    if len(data) > 0:
                        counts = data.value_counts()
                        percentages = (counts / len(data) * 100).round(1)
                        
                        categories = []
                        for cat in counts.index[:10]:  # Ø£ÙˆÙ„ 10 ÙØ¦Ø§Øª
                            categories.append({
                                "Ø§Ù„ÙØ¦Ø©": str(cat),
                                "Ø§Ù„ØªÙƒØ±Ø§Ø±": int(counts[cat]),
                                "Ø§Ù„Ù†Ø³Ø¨Ø©": float(percentages[cat])
                            })
                        
                        results["Ù…ØªØºÙŠØ±Ø§Øª_ÙØ¦ÙˆÙŠØ©"].append({
                            "Ø§Ù„Ù…ØªØºÙŠØ±": column,
                            "Ø¹Ø¯Ø¯_Ø§Ù„ÙØ¦Ø§Øª": int(len(counts)),
                            "Ø§Ù„ØªÙˆØ²ÙŠØ¹": categories
                        })
            except:
                continue
        
        return results


class InferentialAnalyzer:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ÙŠØ©"""
    
    def __init__(self, dataframe):
        self.df = dataframe
    
    def ttest(self, group_var, value_var):
        """Ø§Ø®ØªØ¨Ø§Ø± T Ù„Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©"""
        try:
            clean_df = self.df[[group_var, value_var]].dropna()
            groups = clean_df[group_var].unique()
            
            if len(groups) != 2:
                return {"error": f"ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ {group_var} Ø¹Ù„Ù‰ ÙØ¦ØªÙŠÙ† ÙÙ‚Ø·. Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©: {len(groups)}"}
            
            group1 = clean_df[clean_df[group_var] == groups[0]][value_var]
            group2 = clean_df[clean_df[group_var] == groups[1]][value_var]
            
            t_stat, p_value = stats.ttest_ind(group1, group2)
            
            # Ø­Ø¬Ù… Ø§Ù„Ø£Ø«Ø± (Cohen's d)
            pooled_std = np.sqrt(((len(group1)-1)*group1.std()**2 + (len(group2)-1)*group2.std()**2) / (len(group1)+len(group2)-2))
            cohens_d = (group1.mean() - group2.mean()) / pooled_std if pooled_std != 0 else 0
            
            # Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø­Ø±ÙŠØ©
            df = len(group1) + len(group2) - 2
            
            return {
                "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©_1": {
                    "Ø§Ù„Ø§Ø³Ù…": str(groups[0]),
                    "Ø§Ù„Ø¹Ø¯Ø¯": int(len(group1)),
                    "Ø§Ù„Ù…ØªÙˆØ³Ø·": round(float(group1.mean()), 2),
                    "Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù": round(float(group1.std()), 2)
                },
                "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©_2": {
                    "Ø§Ù„Ø§Ø³Ù…": str(groups[1]),
                    "Ø§Ù„Ø¹Ø¯Ø¯": int(len(group2)),
                    "Ø§Ù„Ù…ØªÙˆØ³Ø·": round(float(group2.mean()), 2),
                    "Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù": round(float(group2.std()), 2)
                },
                "t": round(float(t_stat), 3),
                "df": int(df),
                "p": round(float(p_value), 4),
                "cohens_d": round(float(cohens_d), 3),
                "Ø¯Ø§Ù„": bool(p_value < 0.05),
                "Ù…Ø³ØªÙˆÙ‰_Ø§Ù„Ø¯Ù„Ø§Ù„Ø©": self._get_significance_level(p_value),
                "Ø­Ø¬Ù…_Ø§Ù„Ø£Ø«Ø±": self._interpret_cohens_d(cohens_d)
            }
        except Exception as e:
            return {"error": f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± T: {str(e)}"}
    
    def _get_significance_level(self, p):
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¯Ù„Ø§Ù„Ø©"""
        if p < 0.001:
            return "0.001"
        elif p < 0.01:
            return "0.01"
        elif p < 0.05:
            return "0.05"
        else:
            return "ØºÙŠØ± Ø¯Ø§Ù„"
    
    def _interpret_cohens_d(self, d):
        """ØªÙØ³ÙŠØ± Ø­Ø¬Ù… Ø§Ù„Ø£Ø«Ø±"""
        abs_d = abs(d)
        if abs_d < 0.2:
            return "Ø¶Ø¹ÙŠÙ Ø¬Ø¯Ø§Ù‹"
        elif abs_d < 0.5:
            return "Ø¶Ø¹ÙŠÙ"
        elif abs_d < 0.8:
            return "Ù…ØªÙˆØ³Ø·"
        else:
            return "ÙƒØ¨ÙŠØ±"
    
    def anova(self, dependent, independent):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ø§Ù„Ø£Ø­Ø§Ø¯ÙŠ"""
        try:
            clean_df = self.df[[independent, dependent]].dropna()
            groups = []
            labels = []
            
            for name, group in clean_df.groupby(independent):
                groups.append(group[dependent].values)
                labels.append(name)
            
            if len(groups) < 2:
                return {"error": f"ÙŠØ¬Ø¨ ÙˆØ¬ÙˆØ¯ Ù…Ø¬Ù…ÙˆØ¹ØªÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ ÙÙŠ {independent}"}
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø§ÙŠÙ†
            f_stat, p_value = stats.f_oneway(*groups)
            
            # Ø­Ø³Ø§Ø¨ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª
            grand_mean = clean_df[dependent].mean()
            ss_between = sum([len(g) * (np.mean(g) - grand_mean)**2 for g in groups])
            ss_within = sum([np.sum((g - np.mean(g))**2) for g in groups])
            ss_total = ss_between + ss_within
            
            # Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø­Ø±ÙŠØ©
            df_between = len(groups) - 1
            df_within = len(clean_df) - len(groups)
            df_total = len(clean_df) - 1
            
            # Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª
            ms_between = ss_between / df_between
            ms_within = ss_within / df_within
            
            # Ø­Ø¬Ù… Ø§Ù„Ø£Ø«Ø± (Eta Squared)
            eta_squared = ss_between / ss_total
            
            # ===== NEW: Ø¥Ø­ØµØ§Ø¡Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª =====
            group_descriptives = {}
            for i, name in enumerate(labels):
                group_data = groups[i]
                group_descriptives[str(name)] = {
                    'Ø§Ù„Ø¹Ø¯Ø¯': int(len(group_data)),
                    'Ø§Ù„Ù…ØªÙˆØ³Ø·': round(float(np.mean(group_data)), 2),
                    'Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù_Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ': round(float(np.std(group_data, ddof=1)), 2)
                }
            
            return {
                "N": int(len(clean_df)),
                "Ø¥Ø­ØµØ§Ø¡Ø§Øª_Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª": group_descriptives,
                "Ø¨ÙŠÙ†_Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª": {
                    "Ù…Ø¬Ù…ÙˆØ¹_Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª": round(float(ss_between), 3),
                    "Ø¯Ø±Ø¬Ø§Øª_Ø§Ù„Ø­Ø±ÙŠØ©": int(df_between),
                    "Ù…ØªÙˆØ³Ø·_Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª": round(float(ms_between), 3)
                },
                "Ø¯Ø§Ø®Ù„_Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª": {
                    "Ù…Ø¬Ù…ÙˆØ¹_Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª": round(float(ss_within), 3),
                    "Ø¯Ø±Ø¬Ø§Øª_Ø§Ù„Ø­Ø±ÙŠØ©": int(df_within),
                    "Ù…ØªÙˆØ³Ø·_Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª": round(float(ms_within), 3)
                },
                "Ø§Ù„ÙƒÙ„ÙŠ": {
                    "Ù…Ø¬Ù…ÙˆØ¹_Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª": round(float(ss_total), 3),
                    "Ø¯Ø±Ø¬Ø§Øª_Ø§Ù„Ø­Ø±ÙŠØ©": int(df_total)
                },
                "F": round(float(f_stat), 3),
                "p": round(float(p_value), 4),
                "eta_squared": round(float(eta_squared), 3),
                "Ø¯Ø§Ù„": bool(p_value < 0.05),
                "Ù…Ø³ØªÙˆÙ‰_Ø§Ù„Ø¯Ù„Ø§Ù„Ø©": self._get_significance_level(p_value),
                "Ø­Ø¬Ù…_Ø§Ù„Ø£Ø«Ø±": self._interpret_eta_squared(eta_squared)
            }
        except Exception as e:
            return {"error": f"Ø®Ø·Ø£ ÙÙŠ ANOVA: {str(e)}"}

    def _interpret_eta_squared(self, eta):
        """ØªÙØ³ÙŠØ± Eta Squared"""
        if eta < 0.01:
            return "Ø¶Ø¹ÙŠÙ Ø¬Ø¯Ø§Ù‹"
        elif eta < 0.06:
            return "Ø¶Ø¹ÙŠÙ"
        elif eta < 0.14:
            return "Ù…ØªÙˆØ³Ø·"
        else:
            return "ÙƒØ¨ÙŠØ±"
    
    def correlation(self, variables):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·"""
        try:
            if not variables or len(variables) < 2:
                return {"error": "ÙŠØ¬Ø¨ ØªØ­Ø¯ÙŠØ¯ Ù…ØªØºÙŠØ±ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„"}
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            data = self.df[variables].dropna()
            
            if len(data) < 3:
                return {"error": "Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙ (Ø£Ù‚Ù„ Ù…Ù† 3)"}
            
            # ===== NEW: Ø¥Ø­ØµØ§Ø¡Ø§Øª ÙˆØµÙÙŠØ© =====
            descriptive_stats = {}
            for var in variables:
                descriptive_stats[var] = {
                    'N': int(len(data)),
                    'Mean': round(float(data[var].mean()), 2),
                    'SD': round(float(data[var].std(ddof=1)), 2)
                }
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·
            corr_matrix = data.corr()
            
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ØµÙÙˆÙØ© Ù…Ø¹ Ù‚ÙŠÙ… p - FIXED KEYS
            result_matrix = {}
            significant_results = []
            
            for var1 in variables:
                result_matrix[var1] = {}
                for var2 in variables:
                    if var1 == var2:
                        result_matrix[var1][var2] = {
                            "r": 1.0,
                            "p": 0.0
                        }
                    else:
                        r, p = stats.pearsonr(data[var1], data[var2])
                        result_matrix[var1][var2] = {
                            "r": round(float(r), 3),
                            "p": round(float(p), 4)
                        }
                        
                        # Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¯Ø§Ù„Ø©
                        if p < 0.05 and var1 < var2:  # ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
                            significant_results.append({
                                'var1': var1,
                                'var2': var2,
                                'r': round(float(r), 3),
                                'p': round(float(p), 4),
                                'Ù‚ÙˆØ©': self._interpret_correlation_strength(abs(r))
                            })
            
            return {
                "method": "pearson",
                "N": int(len(data)),
                "Ø¥Ø­ØµØ§Ø¡Ø§Øª_ÙˆØµÙÙŠØ©": descriptive_stats,
                "Ù…ØµÙÙˆÙØ©_Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·": result_matrix,
                "Ù†ØªØ§Ø¦Ø¬_Ø¯Ø§Ù„Ø©": significant_results
            }
        except Exception as e:
            return {"error": f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·: {str(e)}"}
    
    def _interpret_correlation_strength(self, abs_r):
        """ØªÙØ³ÙŠØ± Ù‚ÙˆØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·"""
        if abs_r < 0.3:
            return "Ø¶Ø¹ÙŠÙØ©"
        elif abs_r < 0.5:
            return "Ù…ØªÙˆØ³Ø·Ø©"
        elif abs_r < 0.7:
            return "Ù‚ÙˆÙŠØ©"
        else:
            return "Ù‚ÙˆÙŠØ© Ø¬Ø¯Ø§Ù‹"

    def chi_square(self, var1, var2):
        """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø±Ø¨Ø¹ ÙƒØ§ÙŠ"""
        try:
            # Create contingency table
            contingency = pd.crosstab(self.df[var1], self.df[var2])
            
            # Chi-square test
            chi2, p, dof, expected = stats.chi2_contingency(contingency)
            
            # CramÃ©r's V
            n = contingency.sum().sum()
            min_dim = min(contingency.shape[0], contingency.shape[1]) - 1
            cramers_v = np.sqrt(chi2 / (n * min_dim))
            
            return {
                "N": int(n),
                "var1": var1,
                "var2": var2,
                "chi_square": round(float(chi2), 3),
                "df": int(dof),
                "p": round(float(p), 4),
                "cramers_v": round(float(cramers_v), 3),
                "Ø¯Ø§Ù„": bool(p < 0.05),
                "Ù…Ø³ØªÙˆÙ‰_Ø§Ù„Ø¯Ù„Ø§Ù„Ø©": self._get_significance_level(p),
                "Ù‚ÙˆØ©_Ø§Ù„Ø¹Ù„Ø§Ù‚Ø©": self._interpret_cramers_v(cramers_v),
                "Ø¬Ø¯ÙˆÙ„_Ø§Ù„ØªÙˆØ§ÙÙ‚": contingency.to_dict()
            }
        except Exception as e:
            return {"error": f"Ø®Ø·Ø£ ÙÙŠ Chi-Square: {str(e)}"}

    def _interpret_cramers_v(self, v):
        """ØªÙØ³ÙŠØ± CramÃ©r's V"""
        if v < 0.1:
            return "Ø¶Ø¹ÙŠÙ Ø¬Ø¯Ø§Ù‹"
        elif v < 0.3:
            return "Ø¶Ø¹ÙŠÙ"
        elif v < 0.5:
            return "Ù…ØªÙˆØ³Ø·"
        else:
            return "Ù‚ÙˆÙŠ"
    
    def cronbach_alpha(self, variables):
        """Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø§Ù…Ù„ Ø£Ù„ÙØ§ ÙƒØ±ÙˆÙ†Ø¨Ø§Ø®"""
        try:
            if not variables or len(variables) < 2:
                return {"error": "ÙŠØ¬Ø¨ ØªØ­Ø¯ÙŠØ¯ Ù…ØªØºÙŠØ±ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„"}
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            data = self.df[variables].dropna()
            
            if len(data) < 2:
                return {"error": "Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙ"}
            
            # Ø­Ø³Ø§Ø¨ Cronbach's Alpha
            item_vars = data.var(axis=0, ddof=1)
            total_var = data.sum(axis=1).var(ddof=1)
            n_items = len(variables)
            
            alpha = (n_items / (n_items - 1)) * (1 - item_vars.sum() / total_var)
            
            # Ø¥Ø­ØµØ§Ø¡Ø§Øª Ø§Ù„Ø¨Ù†ÙˆØ¯
            items_stats = []
            for var in variables:
                # Alpha if item deleted
                other_vars = [v for v in variables if v != var]
                if len(other_vars) > 1:
                    temp_data = data[other_vars]
                    temp_item_vars = temp_data.var(axis=0, ddof=1)
                    temp_total_var = temp_data.sum(axis=1).var(ddof=1)
                    n_temp = len(other_vars)
                    alpha_if_deleted = (n_temp / (n_temp - 1)) * (1 - temp_item_vars.sum() / temp_total_var)
                else:
                    alpha_if_deleted = None
                
                items_stats.append({
                    "Ø§Ù„Ø¨Ù†Ø¯": var,
                    "Ø§Ù„Ù…ØªÙˆØ³Ø·": round(float(data[var].mean()), 2),
                    "Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù": round(float(data[var].std()), 2),
                    "Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·_Ù…Ø¹_Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹": round(float(data[var].corr(data.sum(axis=1))), 3),
                    "Ø£Ù„ÙØ§_Ø¥Ø°Ø§_Ø­ÙØ°Ù": round(float(alpha_if_deleted), 3) if alpha_if_deleted else None
                })
            
            return {
                "alpha": round(float(alpha), 3),
                "Ø¹Ø¯Ø¯_Ø§Ù„Ø¨Ù†ÙˆØ¯": n_items,
                "Ø­Ø¬Ù…_Ø§Ù„Ø¹ÙŠÙ†Ø©": int(len(data)),
                "Ø§Ù„ØªØµÙ†ÙŠÙ": self._classify_alpha(alpha),
                "Ø¥Ø­ØµØ§Ø¡Ø§Øª_Ø§Ù„Ø¨Ù†ÙˆØ¯": items_stats
            }
        except Exception as e:
            return {"error": f"Ø®Ø·Ø£ ÙÙŠ Cronbach's Alpha: {str(e)}"}
    
    def _classify_alpha(self, alpha):
        """ØªØµÙ†ÙŠÙ Ù‚ÙŠÙ…Ø© Alpha"""
        if alpha >= 0.9:
            return "Ù…Ù…ØªØ§Ø² (Excellent)"
        elif alpha >= 0.8:
            return "Ø¬ÙŠØ¯ (Good)"
        elif alpha >= 0.7:
            return "Ù…Ù‚Ø¨ÙˆÙ„ (Acceptable)"
        elif alpha >= 0.6:
            return "Ù…Ø´ÙƒÙˆÙƒ ÙÙŠÙ‡ (Questionable)"
        elif alpha >= 0.5:
            return "Ø¶Ø¹ÙŠÙ (Poor)"
        else:
            return "ØºÙŠØ± Ù…Ù‚Ø¨ÙˆÙ„ (Unacceptable)"


class RegressionAnalyzer:
    """Ù…Ø­Ø±Ùƒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±"""
    
    def __init__(self, dataframe):
        self.df = dataframe
    
    def multiple_regression(self, dependent, independents):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯"""
        try:
            if not independents or len(independents) < 1:
                return {"error": "ÙŠØ¬Ø¨ ØªØ­Ø¯ÙŠØ¯ Ù…ØªØºÙŠØ± Ù…Ø³ØªÙ‚Ù„ ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„"}
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            cols = [dependent] + independents
            data = self.df[cols].dropna()
            
            if len(data) < len(independents) + 2:
                return {"error": "Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙ Ù„Ù„Ø§Ù†Ø­Ø¯Ø§Ø±"}
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
            X = data[independents]
            y = data[dependent]
            X = sm.add_constant(X)  # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø«Ø§Ø¨Øª
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±
            model = sm.OLS(y, X).fit()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            coefficients = []
            for i, var in enumerate(['Constant'] + independents):
                coefficients.append({
                    "Ø§Ù„Ù…ØªØºÙŠØ±": var,
                    "Ø§Ù„Ù…Ø¹Ø§Ù…Ù„": round(float(model.params[i]), 3),
                    "Ø§Ù„Ø®Ø·Ø£_Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ": round(float(model.bse[i]), 3),
                    "t": round(float(model.tvalues[i]), 3),
                    "p": round(float(model.pvalues[i]), 4)
                })
            
            return {
                "R": round(float(np.sqrt(model.rsquared)), 3),
                "R2": round(float(model.rsquared), 3),
                "R2_Ø§Ù„Ù…Ø¹Ø¯Ù„": round(float(model.rsquared_adj), 3),
                "Ø§Ù„Ø®Ø·Ø£_Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ": round(float(np.sqrt(model.mse_resid)), 3),
                "F": round(float(model.fvalue), 3),
                "p_model": round(float(model.f_pvalue), 4),
                "Ø¯Ø§Ù„": bool(model.f_pvalue < 0.05),
                "Ù…Ø¹Ø§Ù…Ù„Ø§Øª": coefficients
            }
        except Exception as e:
            return {"error": f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±: {str(e)}"}


class AcademicReportGenerator:
    """Ù…ÙˆÙ„Ø¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© Ø§Ù„Ù†ØµÙŠØ© (ASCII format)"""
    
    def generate(self, results, analysis_type):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ ÙƒØ§Ù…Ù„"""
        if analysis_type == 'descriptive':
            return self._format_descriptive(results)
        elif analysis_type == 'ttest':
            return self._format_ttest(results)
        elif analysis_type == 'anova':
            return self._format_anova(results)
        elif analysis_type == 'correlation':
            return self._format_correlation(results)
        elif analysis_type == 'regression':
            return self._format_regression(results)
        elif analysis_type in ['chi_square', 'chisquare']:
            return self._format_chisquare(results)
        elif analysis_type in ['cronbach', 'cronbach_alpha']:
            return self._format_cronbach(results)
        else:
            return "Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…"
    
    def _format_descriptive(self, r):
        """ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¥Ø­ØµØ§Ø¡ Ø§Ù„ÙˆØµÙÙŠ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ"""
        report = "â•"*55 + "\n"
        report += "        Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ Ø§Ù„ÙˆØµÙÙŠ\n"
        report += "        Descriptive Statistics Analysis\n"
        report += "â•"*55 + "\n\n"
        
        # Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
        if r.get('Ù…ØªØºÙŠØ±Ø§Øª_Ø±Ù‚Ù…ÙŠØ©'):
            report += "ğŸ“Š Ø£ÙˆÙ„Ø§Ù‹: Ø§Ù„Ø¥Ø­ØµØ§Ø¡Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù„Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©\n"
            report += "â”€"*55 + "\n\n"
            
            report += "â”Œ" + "â”€"*70 + "â”\n"
            report += "â”‚ Ø§Ù„Ù…ØªØºÙŠØ±       â”‚  N   â”‚  Mean  â”‚  SD   â”‚  Min  â”‚  Max  â”‚\n"
            report += "â”œ" + "â”€"*70 + "â”¤\n"
            
            for var in r['Ù…ØªØºÙŠØ±Ø§Øª_Ø±Ù‚Ù…ÙŠØ©']:
                report += f"â”‚ {var['Ø§Ù„Ù…ØªØºÙŠØ±']:<14} â”‚ {var['Ø§Ù„Ø¹Ø¯Ø¯']:>4} â”‚ {var['Ø§Ù„Ù…ØªÙˆØ³Ø·']:>6.2f} â”‚ {var['Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù_Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ']:>5.2f} â”‚ {var['Ø£ØµØºØ±_Ù‚ÙŠÙ…Ø©']:>5.2f} â”‚ {var['Ø£ÙƒØ¨Ø±_Ù‚ÙŠÙ…Ø©']:>5.2f} â”‚\n"
            
            report += "â””" + "â”€"*70 + "â”˜\n\n"
        
        return report
    
    def _format_ttest(self, r):
        """ØªÙ‚Ø±ÙŠØ± Ø§Ø®ØªØ¨Ø§Ø± T Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ"""
        if 'error' in r:
            return f"âŒ Ø®Ø·Ø£: {r['error']}"
        
        report = "â•"*55 + "\n"
        report += "   Ø§Ø®ØªØ¨Ø§Ø± T Ù„Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©\n"
        report += "   Independent Samples T-Test\n"
        report += "â•"*55 + "\n\n"
        
        report += "ğŸ“Š Ø£ÙˆÙ„Ø§Ù‹: Ø¥Ø­ØµØ§Ø¡Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª\n"
        report += "â”€"*55 + "\n\n"
        
        report += f"   Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© 1: {r['Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©_1']['Ø§Ù„Ø§Ø³Ù…']}\n"
        report += f"   â€¢ Ø§Ù„Ø¹Ø¯Ø¯ (N) = {r['Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©_1']['Ø§Ù„Ø¹Ø¯Ø¯']}\n"
        report += f"   â€¢ Ø§Ù„Ù…ØªÙˆØ³Ø· (M) = {r['Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©_1']['Ø§Ù„Ù…ØªÙˆØ³Ø·']}\n"
        report += f"   â€¢ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ (SD) = {r['Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©_1']['Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù']}\n\n"
        
        report += f"   Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© 2: {r['Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©_2']['Ø§Ù„Ø§Ø³Ù…']}\n"
        report += f"   â€¢ Ø§Ù„Ø¹Ø¯Ø¯ (N) = {r['Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©_2']['Ø§Ù„Ø¹Ø¯Ø¯']}\n"
        report += f"   â€¢ Ø§Ù„Ù…ØªÙˆØ³Ø· (M) = {r['Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©_2']['Ø§Ù„Ù…ØªÙˆØ³Ø·']}\n"
        report += f"   â€¢ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ (SD) = {r['Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©_2']['Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù']}\n\n"
        
        report += "ğŸ“ˆ Ø«Ø§Ù†ÙŠØ§Ù‹: Ù†ØªØ§Ø¦Ø¬ Ø§Ø®ØªØ¨Ø§Ø± T\n"
        report += "â”€"*55 + "\n\n"
        
        report += f"   â€¢ Ù‚ÙŠÙ…Ø© t = {r['t']}\n"
        report += f"   â€¢ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø­Ø±ÙŠØ© (df) = {r['df']}\n"
        report += f"   â€¢ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¯Ù„Ø§Ù„Ø© (p) = {r['p']}\n"
        report += f"   â€¢ Ø­Ø¬Ù… Ø§Ù„Ø£Ø«Ø± (Cohen's d) = {r['cohens_d']} ({r['Ø­Ø¬Ù…_Ø§Ù„Ø£Ø«Ø±']})\n"
        report += f"   â€¢ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {'Ø¯Ø§Ù„ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Ù‹' if r['Ø¯Ø§Ù„'] else 'ØºÙŠØ± Ø¯Ø§Ù„ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Ù‹'}\n\n"
        
        return report
    
    def _format_anova(self, r):
        """ØªÙ‚Ø±ÙŠØ± ANOVA Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ"""
        if 'error' in r:
            return f"âŒ Ø®Ø·Ø£: {r['error']}"
        
        report = "â•"*55 + "\n"
        report += "     ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ø§Ù„Ø£Ø­Ø§Ø¯ÙŠ - One-Way ANOVA\n"
        report += "â•"*55 + "\n\n"
        
        report += "ğŸ“Š Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø§ÙŠÙ†:\n"
        report += "â”€"*55 + "\n\n"
        
        report += "â”Œ" + "â”€"*70 + "â”\n"
        report += "â”‚ Ù…ØµØ¯Ø± Ø§Ù„ØªØ¨Ø§ÙŠÙ†  â”‚    SS    â”‚  df â”‚    MS   â”‚    F   â”‚   Sig. â”‚\n"
        report += "â”œ" + "â”€"*70 + "â”¤\n"
        
        report += f"â”‚ Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª â”‚ {r['Ø¨ÙŠÙ†_Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª']['Ù…Ø¬Ù…ÙˆØ¹_Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª']:>8.3f} â”‚ {r['Ø¨ÙŠÙ†_Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª']['Ø¯Ø±Ø¬Ø§Øª_Ø§Ù„Ø­Ø±ÙŠØ©']:>3} â”‚ {r['Ø¨ÙŠÙ†_Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª']['Ù…ØªÙˆØ³Ø·_Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª']:>7.3f} â”‚ {r['F']:>6.3f} â”‚ {r['p']:>6.4f} â”‚\n"
        report += f"â”‚ Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øªâ”‚ {r['Ø¯Ø§Ø®Ù„_Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª']['Ù…Ø¬Ù…ÙˆØ¹_Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª']:>8.3f} â”‚ {r['Ø¯Ø§Ø®Ù„_Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª']['Ø¯Ø±Ø¬Ø§Øª_Ø§Ù„Ø­Ø±ÙŠØ©']:>3} â”‚ {r['Ø¯Ø§Ø®Ù„_Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª']['Ù…ØªÙˆØ³Ø·_Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª']:>7.3f} â”‚    -   â”‚    -   â”‚\n"
        report += f"â”‚ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹        â”‚ {r['Ø§Ù„ÙƒÙ„ÙŠ']['Ù…Ø¬Ù…ÙˆØ¹_Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª']:>8.3f} â”‚ {r['Ø§Ù„ÙƒÙ„ÙŠ']['Ø¯Ø±Ø¬Ø§Øª_Ø§Ù„Ø­Ø±ÙŠØ©']:>3} â”‚    -    â”‚    -   â”‚    -   â”‚\n"
        
        report += "â””" + "â”€"*70 + "â”˜\n\n"
        
        report += f"â€¢ Ø­Ø¬Ù… Ø§Ù„Ø£Ø«Ø± (EtaÂ²) = {r['eta_squared']} ({r['Ø­Ø¬Ù…_Ø§Ù„Ø£Ø«Ø±']})\n"
        report += f"â€¢ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {'Ø¯Ø§Ù„ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Ù‹' if r['Ø¯Ø§Ù„'] else 'ØºÙŠØ± Ø¯Ø§Ù„ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Ù‹'}\n\n"
        
        return report
    
    def _format_correlation(self, r):
        """ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ"""
        if 'error' in r:
            return f"âŒ Ø®Ø·Ø£: {r['error']}"
        
        report = "â•"*55 + "\n"
        report += "       ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· - Correlation Analysis\n"
        report += "â•"*55 + "\n\n"
        
        report += f"ğŸ“Š Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· (Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©: {r['Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©'].title()})\n"
        report += f"   Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª: {r['Ø¹Ø¯Ø¯_Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª']}\n"
        report += "â”€"*55 + "\n\n"
        
        # Ø¹Ø±Ø¶ Ù…Ø¨Ø³Ø· Ù„Ù„Ù…ØµÙÙˆÙØ©
        report += "(Ø§Ù†Ø¸Ø± Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø£Ø¹Ù„Ø§Ù‡ Ù„Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„Ø©)\n\n"
        
        return report
    
    def _format_regression(self, r):
        """ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ"""
        if 'error' in r:
            return f"âŒ Ø®Ø·Ø£: {r['error']}"
        
        report = "â•"*55 + "\n"
        report += "  ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯\n"
        report += "  Multiple Regression Analysis\n"
        report += "â•"*55 + "\n\n"
        
        report += "ğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n"
        report += "â”€"*55 + "\n\n"
        
        report += f"   â€¢ R = {r['R']}\n"
        report += f"   â€¢ RÂ² = {r['R2']}\n"
        report += f"   â€¢ RÂ² Ø§Ù„Ù…Ø¹Ø¯Ù„ = {r['R2_Ø§Ù„Ù…Ø¹Ø¯Ù„']}\n"
        report += f"   â€¢ Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ = {r['Ø§Ù„Ø®Ø·Ø£_Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ']}\n\n"
        
        report += "ğŸ“ˆ Ù…Ø¹Ù†ÙˆÙŠØ© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n"
        report += "â”€"*55 + "\n\n"
        
        report += f"   â€¢ F = {r['F']}\n"
        report += f"   â€¢ Sig. = {r['p_model']}\n"
        report += f"   â€¢ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¯Ø§Ù„ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Ù‹' if r['Ø¯Ø§Ù„'] else 'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ø¯Ø§Ù„'}\n\n"
        
        report += "ğŸ“‹ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±:\n"
        report += "â”€"*55 + "\n\n"
        
        for coef in r['Ù…Ø¹Ø§Ù…Ù„Ø§Øª']:
            report += f"   {coef['Ø§Ù„Ù…ØªØºÙŠØ±']}:\n"
            report += f"   â€¢ B = {coef['Ø§Ù„Ù…Ø¹Ø§Ù…Ù„']}, t = {coef.get('t', 'N/A')}, p = {coef['p']}\n\n"
        
        return report
    
    def _format_chisquare(self, r):
        """ØªÙ‚Ø±ÙŠØ± Chi-Square Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ"""
        if 'error' in r:
            return f"âŒ Ø®Ø·Ø£: {r['error']}"
        
        report = "â•"*55 + "\n"
        report += "   Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø±Ø¨Ø¹ ÙƒØ§ÙŠ - Chi-Square Test\n"
        report += "â•"*55 + "\n\n"
        
        report += f"ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ø®ØªØ¨Ø§Ø± Ï‡Â²:\n"
        report += "â”€"*55 + "\n\n"
        
        report += f"   â€¢ Ï‡Â² = {r['chi2']}\n"
        report += f"   â€¢ df = {r['df']}\n"
        report += f"   â€¢ Sig. = {r['p']}\n"
        report += f"   â€¢ CramÃ©r's V = {r['cramers_v']} ({r['Ù‚ÙˆØ©_Ø§Ù„Ø¹Ù„Ø§Ù‚Ø©']})\n"
        report += f"   â€¢ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {'Ø¹Ù„Ø§Ù‚Ø© Ø¯Ø§Ù„Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Ù‹' if r['Ø¯Ø§Ù„'] else 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù„Ø§Ù‚Ø© Ø¯Ø§Ù„Ø©'}\n\n"
        
        return report
    
    def _format_cronbach(self, r):
        """ØªÙ‚Ø±ÙŠØ± Ù…Ø¹Ø§Ù…Ù„ Ø£Ù„ÙØ§ ÙƒØ±ÙˆÙ†Ø¨Ø§Ø® Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ"""
        if 'error' in r:
            return f"âŒ Ø®Ø·Ø£: {r['error']}"
        
        report = "â•"*55 + "\n"
        report += "   Ù…Ø¹Ø§Ù…Ù„ Ø£Ù„ÙØ§ ÙƒØ±ÙˆÙ†Ø¨Ø§Ø® Ù„Ù„Ø«Ø¨Ø§Øª - Cronbach's Alpha\n"
        report += "â•"*55 + "\n\n"
        
        report += "ğŸ“Š Ø£ÙˆÙ„Ø§Ù‹: Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø«Ø¨Ø§Øª Ø§Ù„Ø¹Ø§Ù…\n"
        report += "â”€"*55 + "\n\n"
        
        report += f"   â€¢ Ù…Ø¹Ø§Ù…Ù„ Ø£Ù„ÙØ§ (Î±) = {r['alpha']}\n"
        report += f"   â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ø¨Ù†ÙˆØ¯ = {r['Ø¹Ø¯Ø¯_Ø§Ù„Ø¨Ù†ÙˆØ¯']}\n"
        report += f"   â€¢ Ø­Ø¬Ù… Ø§Ù„Ø¹ÙŠÙ†Ø© (N) = {r['Ø­Ø¬Ù…_Ø§Ù„Ø¹ÙŠÙ†Ø©']}\n"
        report += f"   â€¢ Ø§Ù„ØªØµÙ†ÙŠÙ: {r['Ø§Ù„ØªØµÙ†ÙŠÙ']}\n\n"
        
        report += "ğŸ“‹ Ø«Ø§Ù†ÙŠØ§Ù‹: Ø¬Ø¯ÙˆÙ„ Ø¥Ø­ØµØ§Ø¡Ø§Øª Ø§Ù„Ø¨Ù†ÙˆØ¯\n"
        report += "â”€"*55 + "\n\n"
        
        report += "â”Œ" + "â”€"*70 + "â”\n"
        report += "â”‚ Ø§Ù„Ø¨Ù†Ø¯        â”‚ Ø§Ù„Ù…ØªÙˆØ³Ø· â”‚ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù â”‚ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· â”‚ Î± Ø¥Ø°Ø§ Ø­ÙØ°Ù â”‚\n"
        report += "â”œ" + "â”€"*70 + "â”¤\n"
        
        for item in r['Ø¥Ø­ØµØ§Ø¡Ø§Øª_Ø§Ù„Ø¨Ù†ÙˆØ¯']:
            alpha_del = f"{item['Ø£Ù„ÙØ§_Ø¥Ø°Ø§_Ø­ÙØ°Ù']}" if item['Ø£Ù„ÙØ§_Ø¥Ø°Ø§_Ø­ÙØ°Ù'] is not None else "N/A"
            report += f"â”‚ {item['Ø§Ù„Ø¨Ù†Ø¯']:<12} â”‚ {item['Ø§Ù„Ù…ØªÙˆØ³Ø·']:>8} â”‚ {item['Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù']:>9} â”‚ {item['Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·_Ù…Ø¹_Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹']:>9} â”‚ {alpha_del:>10} â”‚\n"
        
        report += "â””" + "â”€"*70 + "â”˜\n\n"
        
        return report


# ============= API ENDPOINTS =============

@app.route('/')
def home():
    """Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    return jsonify({
        "service": "Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ Ø§Ù„Ø¢Ù„ÙŠ - Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±",
        "version": "2.4",
        "status": "active",
        "endpoints": {
            "/health": "GET - ÙØ­Øµ Ø§Ù„ØµØ­Ø©",
            "/analyze": "POST - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (JSON + ASCII)",
            "/analyze_word": "POST - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Word Document)"
        }
    })


@app.route('/health')
def health():
    """ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ø®Ø§Ø¯Ù…"""
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }), 200


@app.route('/analyze', methods=['POST'])
def analyze():
    """Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„ - JSON Response"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({"success": False, "error": "Ù„Ù… ÙŠØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø¨ÙŠØ§Ù†Ø§Øª"}), 400
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        if 'file_url' not in data or 'analysis_type' not in data:
            return jsonify({"success": False, "error": "file_url Ùˆ analysis_type Ù…Ø·Ù„ÙˆØ¨Ø§Ù†"}), 400
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù
        file_handler = FileHandler()
        df = file_handler.load_file(data['file_url'])
        
        if df is None:
            return jsonify({"success": False, "error": "ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù. ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª"}), 400
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        analysis_type = data['analysis_type'].lower()
        result = None
        
        if analysis_type == 'descriptive':
            analyzer = DescriptiveAnalyzer(df)
            result = analyzer.run_analysis()
        
        elif analysis_type == 'ttest':
            params = data.get('params') or data.get('variables') or {}
            analyzer = InferentialAnalyzer(df)
            result = analyzer.ttest(params.get('group_var'), params.get('value_var'))
        
        elif analysis_type == 'anova':
            params = data.get('params') or data.get('variables') or {}
            analyzer = InferentialAnalyzer(df)
            result = analyzer.anova(params.get('dependent'), params.get('independent'))
        
        elif analysis_type == 'correlation':
            params = data.get('params') or data.get('variables') or {}
            analyzer = InferentialAnalyzer(df)
            result = analyzer.correlation(params.get('variables', []))
        
        elif analysis_type == 'regression':
            params = data.get('params') or data.get('variables') or {}
            analyzer = RegressionAnalyzer(df)
            result = analyzer.multiple_regression(params.get('dependent'), params.get('independents', []))
        
        elif analysis_type == 'chi_square' or analysis_type == 'chisquare':
            params = data.get('params') or data.get('variables') or {}
            analyzer = InferentialAnalyzer(df)
            result = analyzer.chi_square(params.get('var1'), params.get('var2'))
        
        elif analysis_type == 'cronbach' or analysis_type == 'cronbach_alpha':
            params = data.get('params') or data.get('variables') or {}
            analyzer = InferentialAnalyzer(df)
            result = analyzer.cronbach_alpha(params.get('variables', []))
        
        else:
            return jsonify({"success": False, "error": f"Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ '{analysis_type}' ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…"}), 400
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ
        report_gen = AcademicReportGenerator()
        report = report_gen.generate(result, analysis_type)
        
        return jsonify({
            "success": True,
            "analysis_type": analysis_type,
            "timestamp": datetime.now().isoformat(),
            "data": result,
            "report": report
        }), 200
        
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }), 500


@app.route('/analyze_word', methods=['POST'])
def analyze_word():
    """
    Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© - Word Document Response
    
    ÙŠØ³ØªÙ‚Ø¨Ù„ Ù†ÙØ³ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø«Ù„ /analyze Ù„ÙƒÙ† ÙŠØ±Ø¬Ø¹ Ù…Ù„Ù Word Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† JSON
    
    Expected JSON input:
    {
        "file_url": "https://...",
        "analysis_type": "descriptive|ttest|anova|correlation|regression|chi_square|cronbach",
        "params" or "variables": {...}
    }
    
    Returns: Word document (.docx)
    """
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({"success": False, "error": "Ù„Ù… ÙŠØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø¨ÙŠØ§Ù†Ø§Øª"}), 400
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        if 'file_url' not in data or 'analysis_type' not in data:
            return jsonify({"success": False, "error": "file_url Ùˆ analysis_type Ù…Ø·Ù„ÙˆØ¨Ø§Ù†"}), 400
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù
        file_handler = FileHandler()
        df = file_handler.load_file(data['file_url'])
        
        if df is None:
            return jsonify({"success": False, "error": "ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù"}), 400
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        analysis_type = data['analysis_type'].lower()
        result = None
        
        if analysis_type == 'descriptive':
            analyzer = DescriptiveAnalyzer(df)
            result = analyzer.run_analysis()
        
        elif analysis_type == 'ttest':
            params = data.get('params') or data.get('variables') or {}
            analyzer = InferentialAnalyzer(df)
            result = analyzer.ttest(params.get('group_var'), params.get('value_var'))
        
        elif analysis_type == 'anova':
            params = data.get('params') or data.get('variables') or {}
            analyzer = InferentialAnalyzer(df)
            result = analyzer.anova(params.get('dependent'), params.get('independent'))
        
        elif analysis_type == 'correlation':
            params = data.get('params') or data.get('variables') or {}
            analyzer = InferentialAnalyzer(df)
            result = analyzer.correlation(params.get('variables', []))
        
        elif analysis_type == 'regression':
            params = data.get('params') or data.get('variables') or {}
            analyzer = RegressionAnalyzer(df)
            result = analyzer.multiple_regression(params.get('dependent'), params.get('independents', []))
        
        elif analysis_type == 'chi_square' or analysis_type == 'chisquare':
            params = data.get('params') or data.get('variables') or {}
            analyzer = InferentialAnalyzer(df)
            result = analyzer.chi_square(params.get('var1'), params.get('var2'))
        
        elif analysis_type == 'cronbach' or analysis_type == 'cronbach_alpha':
            params = data.get('params') or data.get('variables') or {}
            analyzer = InferentialAnalyzer(df)
            result = analyzer.cronbach_alpha(params.get('variables', []))
        
        else:
            return jsonify({"success": False, "error": f"Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ '{analysis_type}' ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…"}), 400
        
        # ØªÙˆÙ„ÙŠØ¯ Word Document
        word_gen = SPSSWordGenerator()
        
        if analysis_type == 'descriptive':
            word_gen.generate_descriptive(result)
        elif analysis_type == 'ttest':
            word_gen.generate_ttest(result)
        elif analysis_type == 'anova':
            word_gen.generate_anova(result)
        elif analysis_type == 'correlation':
            word_gen.generate_correlation(result)
        elif analysis_type == 'regression':
            word_gen.generate_regression(result)
        elif analysis_type in ['chi_square', 'chisquare']:
            word_gen.generate_chisquare(result)
        elif analysis_type in ['cronbach', 'cronbach_alpha']:
            word_gen.generate_cronbach(result)
        
        # Ø­ÙØ¸ ÙÙŠ Ù…Ù„Ù Ù…Ø¤Ù‚Øª
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.docx')
        word_gen.save(temp_file.name)
        temp_file.close()
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
        filename = f"SPSS_{analysis_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù
        return send_file(
            temp_file.name,
            mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            as_attachment=True,
            download_name=filename
        )
        
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }), 500
    finally:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
        try:
            if 'temp_file' in locals():
                os.unlink(temp_file.name)
        except:
            pass


if __name__ == '__main__':
    import os
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
